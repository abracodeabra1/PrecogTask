{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQzj5lEd_yb9",
        "outputId": "83ea7c0b-aa28-46b2-8c85-a615973fa4e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip -q install fasttext\n",
        "# !pip -q install datasets\n",
        "# from datasets import load_dataset\n",
        "import nltk\n",
        "import string\n",
        "import os\n",
        "import numpy as np\n",
        "import fasttext\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "FASrOYvvA87F"
      },
      "outputs": [],
      "source": [
        "def readingfile(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        next(file)\n",
        "        lines = file.readlines()\n",
        "        # word1_list = []\n",
        "        # word2_list = []\n",
        "        # simlex999_list = []\n",
        "        scores = {}\n",
        "\n",
        "        for line in lines:\n",
        "            data = line.strip().split('\\t')\n",
        "            # print(data)\n",
        "\n",
        "            word1 = data[0]\n",
        "            word2 = data[1]\n",
        "            simlex999 = float(data[3])\n",
        "\n",
        "            # word1_list.append(word1)\n",
        "            # word2_list.append(word2)\n",
        "            key = word1 + \" \" + word2\n",
        "            scores[key] = simlex999\n",
        "            # simlex999_list.append(simlex999)\n",
        "    # for key in scores.keys():\n",
        "    #     print(key, scores[key])\n",
        "    return scores\n",
        "\n",
        "def find_syns(word):\n",
        "    syn = []\n",
        "    ant = []\n",
        "    for synset in wordnet.synsets(word):\n",
        "        for lemma in synset.lemmas():\n",
        "            if '_' not in lemma.name():\n",
        "                syn.append(lemma.name())\n",
        "            if lemma.antonyms():\n",
        "                if '_' not in lemma.antonyms()[0].name():\n",
        "                    ant.append(lemma.antonyms()[0].name())\n",
        "\n",
        "    syn = set(syn)\n",
        "    ant = set(ant)\n",
        "    return syn, ant\n",
        "\n",
        "def cosine_similarity(array1, array2):\n",
        "    dot_product = np.dot(array1, array2)\n",
        "    norm_array1 = np.linalg.norm(array1)\n",
        "    norm_array2 = np.linalg.norm(array2)\n",
        "    cosine_similarity = dot_product / (norm_array1 * norm_array2)\n",
        "    return cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "i2gypyIeA_x3"
      },
      "outputs": [],
      "source": [
        "brown_token_list = list(brown.words())\n",
        "brown_token_list = [word.lower() for word in brown_token_list if not all(char in string.punctuation for char in word)]\n",
        "# dataset = load_dataset(\"mindchain/wikitext2\")\n",
        "# REMOVE THIS\n",
        "# brown_token_list = brown_token_list[:10000]\n",
        "brown_string = \" \".join(brown_token_list)\n",
        "\n",
        "with open(\"data.txt\", \"w\") as file:\n",
        "    file.write(brown_string)\n",
        "\n",
        "\n",
        "test_scores = readingfile(\"SimLex-999.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Loc2qxwTBBWH",
        "outputId": "e4be8d7c-f6bb-4e9d-9c0c-3f3f3b30b2c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.611\n"
          ]
        }
      ],
      "source": [
        "tp, tn, fp, fn = 0, 0, 0, 0\n",
        "decision_bound = 4\n",
        "count = 0\n",
        "\n",
        "if os.path.exists(\"model.bin\"):\n",
        "    model = fasttext.load_model(\"model.bin\")\n",
        "else:\n",
        "    model = fasttext.train_unsupervised(\"data.txt\")\n",
        "    model.save_model(\"model.bin\")\n",
        "\n",
        "\n",
        "# oldie = model.get_word_vector(\"Chomsky\")\n",
        "# age = model.get_word_vector(\"Antarctica\")\n",
        "\n",
        "# print(cosine_similarity(oldie, age))\n",
        "\n",
        "for key in test_scores.keys():\n",
        "    word1, word2 = key.split()\n",
        "    score = test_scores[key]\n",
        "    syn1, ant1 = find_syns(word1)\n",
        "    syn2, ant2 = find_syns(word2)\n",
        "    if (word1 in ant2) or (word2 in ant1):\n",
        "        if (score < decision_bound):\n",
        "            tn += 1\n",
        "        else:\n",
        "            fn += 1\n",
        "            # print(word1, word2, \"fn\")\n",
        "    elif (word1 in syn2) or (word2 in syn1):\n",
        "        if (score >= decision_bound):\n",
        "            tp += 1\n",
        "        else:\n",
        "            fp += 1\n",
        "            # print(word1, word2, \"fp\")\n",
        "    else:\n",
        "        vec1 = model.get_word_vector(word1)\n",
        "        vec2 = model.get_word_vector(word2)\n",
        "        cos_sim = cosine_similarity(vec1, vec2)\n",
        "        if (cos_sim > 0.5):\n",
        "            if (score >= decision_bound):\n",
        "                tp += 1\n",
        "            else:\n",
        "                fp += 1\n",
        "        else:\n",
        "            if (score < decision_bound):\n",
        "                tn += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "\n",
        "if (tp + tn + fp + fn > 0):\n",
        "    print(round((tp + tn)/(fp + fn + tp + tn), 3))\n",
        "    # print(tp, tn, fp, fn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
